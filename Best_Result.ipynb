{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a129c4f",
      "metadata": {
        "id": "2a129c4f"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db96a62",
      "metadata": {
        "id": "6db96a62",
        "outputId": "9b1e5479-2952-453b-9185-663b0c6c12c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA GeForce GTX 1650 with Max-Q Design\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45876b00",
      "metadata": {
        "id": "45876b00"
      },
      "source": [
        "## 2. Improved Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b7f82d90",
      "metadata": {
        "id": "b7f82d90",
        "outputId": "81c4fcbf-483b-48b3-a29f-b46e95640220"
      },
      "outputs": [],
      "source": [
        "def improved_text_cleaning(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '[URL]', text, flags=re.MULTILINE)\n",
        "\n",
        "    text = re.sub(r'@(\\w+)', r'[USER]', text)\n",
        "\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    text = re.sub(r'(\\!)\\1{4,}', r'!!!', text)\n",
        "    text = re.sub(r'(\\?)\\1{4,}', r'???', text)\n",
        "    text = re.sub(r'(\\.)\\{4,}', r'...', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d830792b",
      "metadata": {
        "id": "d830792b"
      },
      "source": [
        "## 3. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "64657f87",
      "metadata": {
        "id": "64657f87",
        "outputId": "ad7c387e-2e9f-4b62-b088-f0c32d47427e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (3468, 9)\n",
            "Test data shape: (1400, 2) \n",
            "\n",
            "Non-Sarcastic (0): 2601 (75.0%)\n",
            "Sarcastic (1):     867 (25.0%)\n",
            "\n",
            "Imbalance Ratio: 3.00\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('iSarcasmEval_EN/train.En.csv', index_col=0)\n",
        "df_test = pd.read_csv('iSarcasmEval_EN/task_A_En_test.csv')\n",
        "\n",
        "print(f\"Training data shape: {df_train.shape}\")\n",
        "print(f\"Test data shape: {df_test.shape} \\n\")\n",
        "\n",
        "\n",
        "class_counts = df_train['sarcastic'].value_counts()\n",
        "print(f\"Non-Sarcastic (0): {class_counts[0]} ({class_counts[0]/len(df_train)*100:.1f}%)\")\n",
        "print(f\"Sarcastic (1):     {class_counts[1]} ({class_counts[1]/len(df_train)*100:.1f}%)\")\n",
        "print(f\"\\nImbalance Ratio: {class_counts[0]/class_counts[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2a72be9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train['text_cleaned'] = df_train['tweet'].apply(improved_text_cleaning)\n",
        "df_test['text_cleaned'] = df_test['tweet'].apply(improved_text_cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd524a5",
      "metadata": {
        "id": "cdd524a5"
      },
      "source": [
        "## 4. Create Dataset Class with Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "23b55ad7",
      "metadata": {
        "id": "23b55ad7",
        "outputId": "b750f93f-4044-4322-c2ec-e167ad17bd37"
      },
      "outputs": [],
      "source": [
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ad2d8c",
      "metadata": {
        "id": "a1ad2d8c"
      },
      "source": [
        "## 5. Configuration - Model and Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5b8c54ea",
      "metadata": {
        "id": "5b8c54ea",
        "outputId": "b3732783-4a32-4169-fa40-e40424c86983"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'model_name': 'roberta-base',  \n",
        "    \n",
        "    'num_epochs': 5, \n",
        "    'batch_size': 16,  \n",
        "    'learning_rate': 1e-5, \n",
        "    'weight_decay': 0.01,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'max_length': 128,\n",
        "\n",
        "    'use_focal_loss': True,\n",
        "    'focal_alpha': [0.25, 0.75],    \n",
        "    'focal_gamma': 2.0,\n",
        "\n",
        "    'validation_split': 0.2,\n",
        "    'random_seed': 42,\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1f0d0d",
      "metadata": {
        "id": "aa1f0d0d"
      },
      "source": [
        "## 6. Focal Loss Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f20f0ea1",
      "metadata": {
        "id": "f20f0ea1",
        "outputId": "4d191518-d99f-471f-ec97-1c47cdb1c91d"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=[0.25, 0.75], gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        if isinstance(alpha, list):\n",
        "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
        "        else:\n",
        "            self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "    \n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        alpha_t = self.alpha.to(inputs.device)[targets]\n",
        "\n",
        "        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8dda2ca",
      "metadata": {
        "id": "c8dda2ca"
      },
      "source": [
        "## 7. Custom Trainer with Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6e67aa87",
      "metadata": {
        "id": "6e67aa87",
        "outputId": "16e9cd44-e1f8-4447-f500-634bab25e5b4"
      },
      "outputs": [],
      "source": [
        "class FocalLossTrainer(Trainer):\n",
        "  \n",
        "    def __init__(self, *args, focal_loss_fn=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.focal_loss_fn = focal_loss_fn\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "     \n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if self.focal_loss_fn is not None:\n",
        "            loss = self.focal_loss_fn(logits, labels)\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    f1 = f1_score(labels, predictions, pos_label=1)\n",
        "    precision = precision_score(labels, predictions, pos_label=1, zero_division=0)\n",
        "    recall = recall_score(labels, predictions, pos_label=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "    return {\n",
        "        'f1_sarcastic': f1,           \n",
        "        'f1_macro': f1_macro,         \n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'accuracy': accuracy,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e4b9bc",
      "metadata": {
        "id": "31e4b9bc"
      },
      "source": [
        "## 8. Prepare Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ef411634",
      "metadata": {
        "id": "ef411634",
        "outputId": "1c575d5e-13f0-4ad1-cc65-10d55a6a92bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples:   2774\n",
            "Validation samples: 694\n",
            "Test samples:       1400\n",
            "\n",
            "Class distribution in splits:\n",
            "Train - Non-sarcastic: 2080,     Sarcastic: 694\n",
            "Val   - Non-sarcastic: 521,      Sarcastic: 173\n",
            "Test  - Non-sarcastic: 1200,     Sarcastic: 200\n"
          ]
        }
      ],
      "source": [
        "X_train_full = df_train['text_cleaned'].values\n",
        "y_train_full = df_train['sarcastic'].values\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full,\n",
        "    y_train_full,\n",
        "    test_size=CONFIG['validation_split'],\n",
        "    random_state=CONFIG['random_seed'],\n",
        "    stratify=y_train_full\n",
        ")\n",
        "\n",
        "X_test = df_test['text_cleaned'].values\n",
        "y_test = df_test['sarcastic'].values\n",
        "\n",
        "print(f\"Training samples:   {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "print(f\"Test samples:       {len(X_test)}\")\n",
        "\n",
        "print(\"\\nClass distribution in splits:\")\n",
        "print(f\"Train - Non-sarcastic: {(y_train==0).sum()},     Sarcastic: {(y_train==1).sum()}\")\n",
        "print(f\"Val   - Non-sarcastic: {(y_val==0).sum()},      Sarcastic: {(y_val==1).sum()}\")\n",
        "print(f\"Test  - Non-sarcastic: {(y_test==0).sum()},     Sarcastic: {(y_test==1).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81653548",
      "metadata": {
        "id": "81653548"
      },
      "source": [
        "## 9. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6aefc4cd",
      "metadata": {
        "id": "6aefc4cd",
        "outputId": "f8372729-22cf-4c29-e1ff-bdc82135dfba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG['model_name'],\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "train_dataset = SarcasmDataset(X_train, y_train, tokenizer, CONFIG['max_length'])\n",
        "val_dataset = SarcasmDataset(X_val, y_val, tokenizer, CONFIG['max_length'])\n",
        "test_dataset = SarcasmDataset(X_test, y_test, tokenizer, CONFIG['max_length'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b199b7b0",
      "metadata": {
        "id": "b199b7b0"
      },
      "source": [
        "## 10. Setup Training with Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4c9a87e6",
      "metadata": {
        "id": "4c9a87e6",
        "outputId": "5732de91-ac70-42ec-c8c4-eaa32fda5eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Alpha: [0.25, 0.75] [non-sarcastic, sarcastic]\n",
            "   Gamma: 2.0\n"
          ]
        }
      ],
      "source": [
        "if CONFIG['use_focal_loss']:\n",
        "    focal_loss = FocalLoss(\n",
        "        alpha=CONFIG['focal_alpha'],\n",
        "        gamma=CONFIG['focal_gamma']\n",
        "    )\n",
        "    print(f\"   Alpha: {CONFIG['focal_alpha']} [non-sarcastic, sarcastic]\")\n",
        "    print(f\"   Gamma: {CONFIG['focal_gamma']}\")\n",
        "else:\n",
        "    focal_loss = None\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=CONFIG['num_epochs'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'] * 2,\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_sarcastic\",  \n",
        "    greater_is_better=True,\n",
        "\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=0,\n",
        "\n",
        "    save_total_limit=2,\n",
        "    seed=CONFIG['random_seed'],\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = FocalLossTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    focal_loss_fn=focal_loss\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4bcebb6",
      "metadata": {
        "id": "c4bcebb6"
      },
      "source": [
        "## 11. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48545a1",
      "metadata": {
        "id": "d48545a1",
        "outputId": "86355c0d-9829-464b-8f6b-af4b88102011"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='108' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [108/870 06:21 < 45:40, 0.28 it/s, Epoch 0.61/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_result = trainer.train()\n",
        "\n",
        "\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "val_results = trainer.evaluate()\n",
        "print(\"\\nValidation Results:\")\n",
        "print(f\"  F1 (Sarcastic):  {val_results['eval_f1_sarcastic']:.4f} ({val_results['eval_f1_sarcastic']*100:.2f}%)\")\n",
        "print(f\"  F1 (Macro):      {val_results['eval_f1_macro']:.4f}\")\n",
        "print(f\"  Precision:       {val_results['eval_precision']:.4f}\")\n",
        "print(f\"  Recall:          {val_results['eval_recall']:.4f}\")\n",
        "print(f\"  Accuracy:        {val_results['eval_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e98de215",
      "metadata": {
        "id": "e98de215"
      },
      "source": [
        "## 12. Test Set Evaluation (Default Threshold 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7b8bd2",
      "metadata": {
        "id": "ef7b8bd2",
        "outputId": "4651669b-94e8-43bb-b239-8e8ef9f12388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEST SET EVALUATION (Threshold 0.5)\n",
            "================================================================================\n",
            "\n",
            "Results with threshold 0.5:\n",
            "  F1 (Sarcastic):  0.4150 (41.50%)\n",
            "  Precision:       0.2876\n",
            "  Recall:          0.7450\n",
            "  Accuracy:        0.7000\n",
            "\n",
            "================================================================================\n",
            "Classification Report:\n",
            "================================================================================\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Non-Sarcastic (0)     0.9422    0.6925    0.7983      1200\n",
            "    Sarcastic (1)     0.2876    0.7450    0.4150       200\n",
            "\n",
            "         accuracy                         0.7000      1400\n",
            "        macro avg     0.6149    0.7188    0.6067      1400\n",
            "     weighted avg     0.8487    0.7000    0.7435      1400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "  True Negatives:  831\n",
            "  False Positives: 369\n",
            "  False Negatives: 51\n",
            "  True Positives:  149\n",
            "\n",
            "Results with threshold 0.5:\n",
            "  F1 (Sarcastic):  0.4150 (41.50%)\n",
            "  Precision:       0.2876\n",
            "  Recall:          0.7450\n",
            "  Accuracy:        0.7000\n",
            "\n",
            "================================================================================\n",
            "Classification Report:\n",
            "================================================================================\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Non-Sarcastic (0)     0.9422    0.6925    0.7983      1200\n",
            "    Sarcastic (1)     0.2876    0.7450    0.4150       200\n",
            "\n",
            "         accuracy                         0.7000      1400\n",
            "        macro avg     0.6149    0.7188    0.6067      1400\n",
            "     weighted avg     0.8487    0.7000    0.7435      1400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "  True Negatives:  831\n",
            "  False Positives: 369\n",
            "  False Negatives: 51\n",
            "  True Positives:  149\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "all_probabilities = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_dataset), CONFIG['batch_size'] * 2):\n",
        "        batch_indices = range(i, min(i + CONFIG['batch_size'] * 2, len(test_dataset)))\n",
        "\n",
        "        batch_input_ids = torch.stack([test_dataset[j]['input_ids'] for j in batch_indices]).to(device)\n",
        "        batch_attention_mask = torch.stack([test_dataset[j]['attention_mask'] for j in batch_indices]).to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_probabilities.extend(probs.cpu().numpy())\n",
        "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "        all_predictions.extend(preds)\n",
        "\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "f1_sarc = f1_score(y_test, all_predictions, pos_label=1)\n",
        "precision_sarc = precision_score(y_test, all_predictions, pos_label=1)\n",
        "recall_sarc = recall_score(y_test, all_predictions, pos_label=1)\n",
        "accuracy = accuracy_score(y_test, all_predictions)\n",
        "\n",
        "print(f\"\\nResults with threshold 0.5:\")\n",
        "print(f\"  F1 (Sarcastic):  {f1_sarc:.4f} ({f1_sarc*100:.2f}%)\")\n",
        "print(f\"  Precision:       {precision_sarc:.4f}\")\n",
        "print(f\"  Recall:          {recall_sarc:.4f}\")\n",
        "print(f\"  Accuracy:        {accuracy:.4f}\")\n",
        "\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    all_predictions,\n",
        "    target_names=['Non-Sarcastic (0)', 'Sarcastic (1)'],\n",
        "    digits=4\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5b2ea0",
      "metadata": {
        "id": "6b5b2ea0"
      },
      "source": [
        "## 13. Threshold Optimization for Maximum F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acdbc863",
      "metadata": {
        "id": "acdbc863",
        "outputId": "08b945a2-fbcc-4596-9038-68de87c01593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "THRESHOLD OPTIMIZATION\n",
            "================================================================================\n",
            "Finding optimal threshold to maximize F1 score for SARCASTIC class...\n",
            "\n",
            "Threshold    F1         Precision    Recall     Accuracy  \n",
            "------------------------------------------------------------\n",
            "0.10         0.2782     0.1622       0.9750     0.2771     ‚Üê BEST\n",
            "0.15         0.2966     0.1758       0.9500     0.3564     ‚Üê BEST\n",
            "0.20         0.3112     0.1871       0.9250     0.4150     ‚Üê BEST\n",
            "0.25         0.3257     0.1993       0.8900     0.4736     ‚Üê BEST\n",
            "0.30         0.3448     0.2147       0.8750     0.5250     ‚Üê BEST\n",
            "0.35         0.3571     0.2267       0.8400     0.5679     ‚Üê BEST\n",
            "0.40         0.3744     0.2426       0.8200     0.6086     ‚Üê BEST\n",
            "0.45         0.3865     0.2575       0.7750     0.6486     ‚Üê BEST\n",
            "0.50         0.4150     0.2876       0.7450     0.7000     ‚Üê BEST\n",
            "0.55         0.4319     0.3113       0.7050     0.7350     ‚Üê BEST\n",
            "0.60         0.4355     0.3275       0.6500     0.7593     ‚Üê BEST\n",
            "0.65         0.4624     0.3762       0.6000     0.8007     ‚Üê BEST\n",
            "0.70         0.4967     0.4431       0.5650     0.8364     ‚Üê BEST\n",
            "0.75         0.4960     0.5251       0.4700     0.8636    \n",
            "0.80         0.4505     0.5639       0.3750     0.8693    \n",
            "0.85         0.4029     0.7179       0.2800     0.8814    \n",
            "\n",
            "================================================================================\n",
            "‚úÖ OPTIMAL THRESHOLD: 0.70\n",
            "   Maximum F1-Score: 0.4967 (49.67%)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "best_threshold = 0.5\n",
        "best_f1 = 0\n",
        "threshold_results = []\n",
        "\n",
        "print(f\"{'Threshold':<12} {'F1':<10} {'Precision':<12} {'Recall':<10} {'Accuracy':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for threshold in thresholds:\n",
        "    preds = (all_probabilities[:, 1] >= threshold).astype(int)\n",
        "\n",
        "    f1 = f1_score(y_test, preds, pos_label=1)\n",
        "    precision = precision_score(y_test, preds, pos_label=1, zero_division=0)\n",
        "    recall = recall_score(y_test, preds, pos_label=1)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "\n",
        "    threshold_results.append({\n",
        "        'threshold': threshold,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'accuracy': acc\n",
        "    })\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "\n",
        "    marker = \" ‚Üê BEST\" if threshold == best_threshold and f1 == best_f1 else \"\"\n",
        "    print(f\"{threshold:<12.2f} {f1:<10.4f} {precision:<12.4f} {recall:<10.4f} {acc:<10.4f}{marker}\")\n",
        "\n",
        "\n",
        "print(f\"OPTIMAL THRESHOLD: {best_threshold:.2f}\")\n",
        "print(f\"Maximum F1-Score: {best_f1:.4f} ({best_f1*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2128a00d",
      "metadata": {
        "id": "2128a00d"
      },
      "source": [
        "## 14. Final Results with Optimal Threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d949e1",
      "metadata": {
        "id": "01d949e1",
        "outputId": "218d77f5-31df-49b9-c341-f6e811f6d13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FINAL RESULTS (Optimal Threshold: 0.70)\n",
            "================================================================================\n",
            "\n",
            "üìä Performance Metrics:\n",
            "  F1 (Sarcastic):  0.4967 (49.67%) ‚≠ê\n",
            "  F1 (Macro):      0.6995 (69.95%)\n",
            "  Precision:       0.4431\n",
            "  Recall:          0.5650\n",
            "  Accuracy:        0.8364\n",
            "\n",
            "================================================================================\n",
            "Detailed Classification Report:\n",
            "================================================================================\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Non-Sarcastic (0)     0.9240    0.8817    0.9023      1200\n",
            "    Sarcastic (1)     0.4431    0.5650    0.4967       200\n",
            "\n",
            "         accuracy                         0.8364      1400\n",
            "        macro avg     0.6836    0.7233    0.6995      1400\n",
            "     weighted avg     0.8553    0.8364    0.8444      1400\n",
            "\n",
            "\n",
            "üìä Confusion Matrix:\n",
            "  True Negatives:  1058 (correct non-sarcastic)\n",
            "  False Positives: 142 (non-sarcastic classified as sarcastic)\n",
            "  False Negatives: 87 (sarcastic classified as non-sarcastic)\n",
            "  True Positives:  113 (correct sarcastic) ‚≠ê\n",
            "\n",
            "üí° Minority Class Analysis:\n",
            "  Sarcastic samples in test: 200\n",
            "  Correctly identified: 113 (56.5%)\n",
            "  Precision: 44.3%\n"
          ]
        }
      ],
      "source": [
        "final_predictions = (all_probabilities[:, 1] >= best_threshold).astype(int)\n",
        "\n",
        "final_f1 = f1_score(y_test, final_predictions, pos_label=1)\n",
        "final_precision = precision_score(y_test, final_predictions, pos_label=1)\n",
        "final_recall = recall_score(y_test, final_predictions, pos_label=1)\n",
        "final_accuracy = accuracy_score(y_test, final_predictions)\n",
        "final_f1_macro = f1_score(y_test, final_predictions, average='macro')\n",
        "\n",
        "\n",
        "print(f\"FINAL RESULTS (Optimal Threshold: {best_threshold:.2f})\")\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  F1 (Sarcastic):  {final_f1:.4f} ({final_f1*100:.2f}%)\")\n",
        "print(f\"  F1 (Macro):      {final_f1_macro:.4f} ({final_f1_macro*100:.2f}%)\")\n",
        "print(f\"  Precision:       {final_precision:.4f}\")\n",
        "print(f\"  Recall:          {final_recall:.4f}\")\n",
        "print(f\"  Accuracy:        {final_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    final_predictions,\n",
        "    target_names=['Non-Sarcastic (0)', 'Sarcastic (1)'],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "\n",
        "minority_recall = final_cm[1][1] / (final_cm[1][0] + final_cm[1][1])\n",
        "minority_precision = final_cm[1][1] / (final_cm[0][1] + final_cm[1][1]) if (final_cm[0][1] + final_cm[1][1]) > 0 else 0\n",
        "\n",
        "print(f\"\\nüí° Minority Class Analysis:\")\n",
        "print(f\"  Sarcastic samples in test: {final_cm[1][0] + final_cm[1][1]}\")\n",
        "print(f\"  Correctly identified: {final_cm[1][1]} ({minority_recall*100:.1f}%)\")\n",
        "print(f\"  Precision: {minority_precision*100:.1f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
