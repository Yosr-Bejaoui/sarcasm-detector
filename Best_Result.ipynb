{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a129c4f",
      "metadata": {
        "id": "2a129c4f"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db96a62",
      "metadata": {
        "id": "6db96a62",
        "outputId": "9b1e5479-2952-453b-9185-663b0c6c12c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45876b00",
      "metadata": {
        "id": "45876b00"
      },
      "source": [
        "## 2. Improved Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b7f82d90",
      "metadata": {
        "id": "b7f82d90",
        "outputId": "81c4fcbf-483b-48b3-a29f-b46e95640220"
      },
      "outputs": [],
      "source": [
        "def improved_text_cleaning(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '[URL]', text, flags=re.MULTILINE)\n",
        "\n",
        "    text = re.sub(r'@(\\w+)', r'[USER]', text)\n",
        "\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    text = re.sub(r'(\\!)\\1{4,}', r'!!!', text)\n",
        "    text = re.sub(r'(\\?)\\1{4,}', r'???', text)\n",
        "    text = re.sub(r'(\\.)\\{4,}', r'...', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b107b5cb",
      "metadata": {},
      "source": [
        "## 3. Data Augmentation for Minority Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d7aad9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def augment_sarcastic_samples(texts, labels, augment_factor=2.5):\n",
        "    \"\"\"\n",
        "    Augment sarcastic samples using simple text variations\n",
        "    \"\"\"\n",
        "    augmented_texts = list(texts)\n",
        "    augmented_labels = list(labels)\n",
        "    \n",
        "    sarcastic_indices = [i for i, label in enumerate(labels) if label == 1]\n",
        "    \n",
        "    # Calculate how many augmentations we need\n",
        "    num_augmentations = int(len(sarcastic_indices) * (augment_factor - 1))\n",
        "    \n",
        "    for _ in range(num_augmentations):\n",
        "        idx = random.choice(sarcastic_indices)\n",
        "        original_text = texts[idx]\n",
        "        \n",
        "        # Simple augmentation: random word order changes or repetitions\n",
        "        words = original_text.split()\n",
        "        \n",
        "        # Randomly choose an augmentation type\n",
        "        aug_type = random.choice(['swap', 'duplicate', 'remove'])\n",
        "        \n",
        "        if aug_type == 'swap' and len(words) > 3:\n",
        "            # Swap two random words\n",
        "            i, j = random.sample(range(len(words)), 2)\n",
        "            words[i], words[j] = words[j], words[i]\n",
        "        elif aug_type == 'duplicate' and len(words) > 2:\n",
        "            # Duplicate a random word\n",
        "            i = random.randint(0, len(words) - 1)\n",
        "            words.insert(i, words[i])\n",
        "        elif aug_type == 'remove' and len(words) > 4:\n",
        "            # Remove a random stop word\n",
        "            stop_words = ['the', 'a', 'an', 'is', 'are', 'was', 'were']\n",
        "            removable = [i for i, w in enumerate(words) if w.lower() in stop_words]\n",
        "            if removable:\n",
        "                words.pop(random.choice(removable))\n",
        "        \n",
        "        augmented_text = ' '.join(words)\n",
        "        augmented_texts.append(augmented_text)\n",
        "        augmented_labels.append(1)\n",
        "    \n",
        "    return np.array(augmented_texts), np.array(augmented_labels)\n",
        "\n",
        "print(\"Data augmentation function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d830792b",
      "metadata": {
        "id": "d830792b"
      },
      "source": [
        "## 4. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "64657f87",
      "metadata": {
        "id": "64657f87",
        "outputId": "ad7c387e-2e9f-4b62-b088-f0c32d47427e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'iSarcasmEval_EN\\\\train.En.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-487992196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'iSarcasmEval_EN\\train.En.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'iSarcasmEval_EN\\task_A_En_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training data shape: {df_train.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test data shape: {df_test.shape} \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iSarcasmEval_EN\\\\train.En.csv'"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv(r'iSarcasmEval_EN\\train.En.csv', index_col=0)\n",
        "df_test = pd.read_csv(r'iSarcasmEval_EN\\task_A_En_test.csv')\n",
        "\n",
        "print(f\"Training data shape: {df_train.shape}\")\n",
        "print(f\"Test data shape: {df_test.shape} \\n\")\n",
        "\n",
        "\n",
        "class_counts = df_train['sarcastic'].value_counts()\n",
        "print(f\"Non-Sarcastic (0): {class_counts[0]} ({class_counts[0]/len(df_train)*100:.1f}%)\")\n",
        "print(f\"Sarcastic (1):     {class_counts[1]} ({class_counts[1]/len(df_train)*100:.1f}%)\")\n",
        "print(f\"\\nImbalance Ratio: {class_counts[0]/class_counts[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a72be9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train['text_cleaned'] = df_train['tweet'].apply(improved_text_cleaning)\n",
        "df_test['text_cleaned'] = df_test['tweet'].apply(improved_text_cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd524a5",
      "metadata": {
        "id": "cdd524a5"
      },
      "source": [
        "## 5. Create Dataset Class with Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b55ad7",
      "metadata": {
        "id": "23b55ad7",
        "outputId": "b750f93f-4044-4322-c2ec-e167ad17bd37"
      },
      "outputs": [],
      "source": [
        "class SarcasmDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ad2d8c",
      "metadata": {
        "id": "a1ad2d8c"
      },
      "source": [
        "## 6. Configuration - Model and Training Parameters (OPTIMIZED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8c54ea",
      "metadata": {
        "id": "5b8c54ea",
        "outputId": "b3732783-4a32-4169-fa40-e40424c86983"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    # Twitter-trained RoBERTa - better for social media sarcasm\n",
        "    'model_name': 'cardiffnlp/twitter-roberta-base-sentiment-latest',  \n",
        "    \n",
        "    # Training parameters optimized for imbalanced data\n",
        "    'num_epochs': 8,  # More epochs with early stopping\n",
        "    'batch_size': 8,  # Smaller batch size for better generalization\n",
        "    'gradient_accumulation_steps': 4,  # Effective batch size of 32\n",
        "    'learning_rate': 2e-5,  # Higher LR for better minority class learning\n",
        "    'weight_decay': 0.05,  # Increased regularization\n",
        "    'warmup_ratio': 0.15,  # Longer warmup\n",
        "    'max_length': 128,\n",
        "\n",
        "    # Focal loss optimized for sarcasm (minority) class\n",
        "    'use_focal_loss': True,\n",
        "    'focal_alpha': [0.15, 0.85],  # Much higher weight for sarcastic class\n",
        "    'focal_gamma': 3.0,  # Stronger focus on hard examples\n",
        "\n",
        "\n",
        "    # Data augmentation}\n",
        "\n",
        "    'augment_factor': 2.5,  # Augment sarcastic samples by 2.5x\n",
        "\n",
        "        'random_seed': 42,\n",
        "\n",
        "    # Validation and regularization    'early_stopping_patience': 3,\n",
        "    'validation_split': 0.15,  # Smaller validation split to keep more training data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1f0d0d",
      "metadata": {
        "id": "aa1f0d0d"
      },
      "source": [
        "## 7. Focal Loss Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f20f0ea1",
      "metadata": {
        "id": "f20f0ea1",
        "outputId": "4d191518-d99f-471f-ec97-1c47cdb1c91d"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=[0.25, 0.75], gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        if isinstance(alpha, list):\n",
        "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
        "        else:\n",
        "            self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "    \n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        alpha_t = self.alpha.to(inputs.device)[targets]\n",
        "\n",
        "        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8dda2ca",
      "metadata": {
        "id": "c8dda2ca"
      },
      "source": [
        "## 8. Custom Trainer with Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e67aa87",
      "metadata": {
        "id": "6e67aa87",
        "outputId": "16e9cd44-e1f8-4447-f500-634bab25e5b4"
      },
      "outputs": [],
      "source": [
        "class FocalLossTrainer(Trainer):\n",
        "  \n",
        "    def __init__(self, *args, focal_loss_fn=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.focal_loss_fn = focal_loss_fn\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "     \n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if self.focal_loss_fn is not None:\n",
        "            loss = self.focal_loss_fn(logits, labels)\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    f1 = f1_score(labels, predictions, pos_label=1)\n",
        "    precision = precision_score(labels, predictions, pos_label=1, zero_division=0)\n",
        "    recall = recall_score(labels, predictions, pos_label=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "    return {\n",
        "        'f1_sarcastic': f1,           \n",
        "        'f1_macro': f1_macro,         \n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'accuracy': accuracy,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e4b9bc",
      "metadata": {
        "id": "31e4b9bc"
      },
      "source": [
        "## 9. Prepare Data for Training with Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef411634",
      "metadata": {
        "id": "ef411634",
        "outputId": "1c575d5e-13f0-4ad1-cc65-10d55a6a92bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples:   2774\n",
            "Validation samples: 694\n",
            "Test samples:       1400\n",
            "\n",
            "Class distribution in splits:\n",
            "Train - Non-sarcastic: 2080,     Sarcastic: 694\n",
            "Val   - Non-sarcastic: 521,      Sarcastic: 173\n",
            "Test  - Non-sarcastic: 1200,     Sarcastic: 200\n"
          ]
        }
      ],
      "source": [
        "X_train_full = df_train['text_cleaned'].values\n",
        "y_train_full = df_train['sarcastic'].values\n",
        "\n",
        "# First split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full,\n",
        "    y_train_full,\n",
        "    test_size=CONFIG['validation_split'],\n",
        "    random_state=CONFIG['random_seed'],\n",
        "    stratify=y_train_full\n",
        ")\n",
        "\n",
        "print(\"Original training data:\")\n",
        "print(f\"  Non-sarcastic: {(y_train==0).sum()}\")\n",
        "print(f\"  Sarcastic: {(y_train==1).sum()}\")\n",
        "print(f\"  Imbalance ratio: {(y_train==0).sum()/(y_train==1).sum():.2f}\")\n",
        "\n",
        "# Augment only the training set (not validation)\n",
        "X_train, y_train = augment_sarcastic_samples(\n",
        "    X_train, \n",
        "    y_train, \n",
        "    augment_factor=CONFIG['augment_factor']\n",
        "\n",
        ")print(f\"Test samples:       {len(X_test)}\")\n",
        "\n",
        "print(f\"Validation samples: {len(X_val)}\")\n",
        "\n",
        "print(f\"\\nAfter augmentation:\")print(f\"Training samples:   {len(X_train)}\")\n",
        "\n",
        "print(f\"  Non-sarcastic: {(y_train==0).sum()}\")print(f\"\\nFinal data splits:\")\n",
        "\n",
        "print(f\"  Sarcastic: {(y_train==1).sum()}\")\n",
        "\n",
        "print(f\"  New imbalance ratio: {(y_train==0).sum()/(y_train==1).sum():.2f}\")y_test = df_test['sarcastic'].values\n",
        "\n",
        "X_test = df_test['text_cleaned'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81653548",
      "metadata": {
        "id": "81653548"
      },
      "source": [
        "## 10. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aefc4cd",
      "metadata": {
        "id": "6aefc4cd",
        "outputId": "f8372729-22cf-4c29-e1ff-bdc82135dfba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    CONFIG['model_name'],\n",
        "    num_labels=2,\n",
        "    hidden_dropout_prob=0.2,  # Increased dropout\n",
        "    attention_probs_dropout_prob=0.2,  # Increased dropout\n",
        "    classifier_dropout=0.2  # Classifier dropout\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded: {CONFIG['model_name']}\")\n",
        "\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")test_dataset = SarcasmDataset(X_test, y_test, tokenizer, CONFIG['max_length'])\n",
        "\n",
        "val_dataset = SarcasmDataset(X_val, y_val, tokenizer, CONFIG['max_length'])\n",
        "train_dataset = SarcasmDataset(X_train, y_train, tokenizer, CONFIG['max_length'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b199b7b0",
      "metadata": {
        "id": "b199b7b0"
      },
      "source": [
        "## 11. Setup Training with Focal Loss and Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9a87e6",
      "metadata": {
        "id": "4c9a87e6",
        "outputId": "5732de91-ac70-42ec-c8c4-eaa32fda5eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Alpha: [0.25, 0.75] [non-sarcastic, sarcastic]\n",
            "   Gamma: 2.0\n"
          ]
        }
      ],
      "source": [
        "if CONFIG['use_focal_loss']:\n",
        "    focal_loss = FocalLoss(\n",
        "        alpha=CONFIG['focal_alpha'],\n",
        "        gamma=CONFIG['focal_gamma']\n",
        "    )\n",
        "    print(f\"Using Focal Loss:\")\n",
        "    print(f\"   Alpha: {CONFIG['focal_alpha']} [non-sarcastic, sarcastic]\")\n",
        "    print(f\"   Gamma: {CONFIG['focal_gamma']}\")\n",
        "else:\n",
        "    focal_loss = None\n",
        "\n",
        "output_dir = './results_optimized'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=CONFIG['num_epochs'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'] * 2,\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    \n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "    max_grad_norm=1.0,  # Gradient clipping\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_sarcastic\",  \n",
        "    greater_is_better=True,\n",
        "\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=0,\n",
        "\n",
        "    save_total_limit=2,\n",
        "    seed=CONFIG['random_seed'],\n",
        "    report_to=\"none\",\n",
        "    \n",
        "    # Label smoothing for regularization\n",
        "\n",
        "    label_smoothing_factor=0.1,)\n",
        "\n",
        ")    focal_loss_fn=focal_loss\n",
        "\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        "print(f\"\\nTraining configuration:\")    eval_dataset=val_dataset,\n",
        "\n",
        "print(f\"  Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")    train_dataset=train_dataset,\n",
        "\n",
        "print(f\"  Learning rate: {CONFIG['learning_rate']}\")    args=training_args,\n",
        "\n",
        "print(f\"  Weight decay: {CONFIG['weight_decay']}\")    model=model,\n",
        "\n",
        "print(f\"  Max epochs: {CONFIG['num_epochs']}\")trainer = FocalLossTrainer(\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4bcebb6",
      "metadata": {
        "id": "c4bcebb6"
      },
      "source": [
        "## 12. Train the Model with Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48545a1",
      "metadata": {
        "id": "d48545a1",
        "outputId": "86355c0d-9829-464b-8f6b-af4b88102011"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='207' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [207/870 12:59 < 42:02, 0.26 it/s, Epoch 1.18/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Sarcastic</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.064300</td>\n",
              "      <td>0.064450</td>\n",
              "      <td>0.426357</td>\n",
              "      <td>0.543454</td>\n",
              "      <td>0.320700</td>\n",
              "      <td>0.635838</td>\n",
              "      <td>0.573487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"Starting training...\\n\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "val_results = trainer.evaluate()\n",
        "print(\"\\nValidation Results:\")\n",
        "print(f\"  F1 (Sarcastic):  {val_results['eval_f1_sarcastic']:.4f} ({val_results['eval_f1_sarcastic']*100:.2f}%)\")\n",
        "print(f\"  F1 (Macro):      {val_results['eval_f1_macro']:.4f}\")\n",
        "\n",
        "print(f\"  Precision:       {val_results['eval_precision']:.4f}\")    print(\"   - Increasing augmentation factor\")\n",
        "\n",
        "print(f\"  Recall:          {val_results['eval_recall']:.4f}\")    print(\"   - Adjusting focal loss parameters\")\n",
        "\n",
        "print(f\"  Accuracy:        {val_results['eval_accuracy']:.4f}\")    print(\"   - Running more epochs\")\n",
        "\n",
        "    print(\"\\n‚ö†Ô∏è Warning: F1 score is still below target. Consider:\")\n",
        "\n",
        "# Check for overfittingif val_results['eval_f1_sarcastic'] < 0.45:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e98de215",
      "metadata": {
        "id": "e98de215"
      },
      "source": [
        "## 13. Test Set Evaluation (Default Threshold 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7b8bd2",
      "metadata": {
        "id": "ef7b8bd2",
        "outputId": "4651669b-94e8-43bb-b239-8e8ef9f12388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TEST SET EVALUATION (Threshold 0.5)\n",
            "================================================================================\n",
            "\n",
            "Results with threshold 0.5:\n",
            "  F1 (Sarcastic):  0.4150 (41.50%)\n",
            "  Precision:       0.2876\n",
            "  Recall:          0.7450\n",
            "  Accuracy:        0.7000\n",
            "\n",
            "================================================================================\n",
            "Classification Report:\n",
            "================================================================================\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Non-Sarcastic (0)     0.9422    0.6925    0.7983      1200\n",
            "    Sarcastic (1)     0.2876    0.7450    0.4150       200\n",
            "\n",
            "         accuracy                         0.7000      1400\n",
            "        macro avg     0.6149    0.7188    0.6067      1400\n",
            "     weighted avg     0.8487    0.7000    0.7435      1400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "  True Negatives:  831\n",
            "  False Positives: 369\n",
            "  False Negatives: 51\n",
            "  True Positives:  149\n",
            "\n",
            "Results with threshold 0.5:\n",
            "  F1 (Sarcastic):  0.4150 (41.50%)\n",
            "  Precision:       0.2876\n",
            "  Recall:          0.7450\n",
            "  Accuracy:        0.7000\n",
            "\n",
            "================================================================================\n",
            "Classification Report:\n",
            "================================================================================\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Non-Sarcastic (0)     0.9422    0.6925    0.7983      1200\n",
            "    Sarcastic (1)     0.2876    0.7450    0.4150       200\n",
            "\n",
            "         accuracy                         0.7000      1400\n",
            "        macro avg     0.6149    0.7188    0.6067      1400\n",
            "     weighted avg     0.8487    0.7000    0.7435      1400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "  True Negatives:  831\n",
            "  False Positives: 369\n",
            "  False Negatives: 51\n",
            "  True Positives:  149\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "all_probabilities = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_dataset), CONFIG['batch_size'] * 2):\n",
        "        batch_indices = range(i, min(i + CONFIG['batch_size'] * 2, len(test_dataset)))\n",
        "\n",
        "        batch_input_ids = torch.stack([test_dataset[j]['input_ids'] for j in batch_indices]).to(device)\n",
        "        batch_attention_mask = torch.stack([test_dataset[j]['attention_mask'] for j in batch_indices]).to(device)\n",
        "\n",
        "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
        "        probs = F.softmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_probabilities.extend(probs.cpu().numpy())\n",
        "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "        all_predictions.extend(preds)\n",
        "\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "f1_sarc = f1_score(y_test, all_predictions, pos_label=1)\n",
        "precision_sarc = precision_score(y_test, all_predictions, pos_label=1)\n",
        "recall_sarc = recall_score(y_test, all_predictions, pos_label=1)\n",
        "accuracy = accuracy_score(y_test, all_predictions)\n",
        "\n",
        "print(f\"\\nResults with threshold 0.5:\")\n",
        "print(f\"  F1 (Sarcastic):  {f1_sarc:.4f} ({f1_sarc*100:.2f}%)\")\n",
        "print(f\"  Precision:       {precision_sarc:.4f}\")\n",
        "print(f\"  Recall:          {recall_sarc:.4f}\")\n",
        "print(f\"  Accuracy:        {accuracy:.4f}\")\n",
        "\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    all_predictions,\n",
        "    target_names=['Non-Sarcastic (0)', 'Sarcastic (1)'],\n",
        "    digits=4\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5b2ea0",
      "metadata": {
        "id": "6b5b2ea0"
      },
      "source": [
        "## 14. Threshold Optimization for Maximum F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acdbc863",
      "metadata": {
        "id": "acdbc863",
        "outputId": "08b945a2-fbcc-4596-9038-68de87c01593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "THRESHOLD OPTIMIZATION\n",
            "================================================================================\n",
            "Finding optimal threshold to maximize F1 score for SARCASTIC class...\n",
            "\n",
            "Threshold    F1         Precision    Recall     Accuracy  \n",
            "------------------------------------------------------------\n",
            "0.10         0.2782     0.1622       0.9750     0.2771     ‚Üê BEST\n",
            "0.15         0.2966     0.1758       0.9500     0.3564     ‚Üê BEST\n",
            "0.20         0.3112     0.1871       0.9250     0.4150     ‚Üê BEST\n",
            "0.25         0.3257     0.1993       0.8900     0.4736     ‚Üê BEST\n",
            "0.30         0.3448     0.2147       0.8750     0.5250     ‚Üê BEST\n",
            "0.35         0.3571     0.2267       0.8400     0.5679     ‚Üê BEST\n",
            "0.40         0.3744     0.2426       0.8200     0.6086     ‚Üê BEST\n",
            "0.45         0.3865     0.2575       0.7750     0.6486     ‚Üê BEST\n",
            "0.50         0.4150     0.2876       0.7450     0.7000     ‚Üê BEST\n",
            "0.55         0.4319     0.3113       0.7050     0.7350     ‚Üê BEST\n",
            "0.60         0.4355     0.3275       0.6500     0.7593     ‚Üê BEST\n",
            "0.65         0.4624     0.3762       0.6000     0.8007     ‚Üê BEST\n",
            "0.70         0.4967     0.4431       0.5650     0.8364     ‚Üê BEST\n",
            "0.75         0.4960     0.5251       0.4700     0.8636    \n",
            "0.80         0.4505     0.5639       0.3750     0.8693    \n",
            "0.85         0.4029     0.7179       0.2800     0.8814    \n",
            "\n",
            "================================================================================\n",
            "‚úÖ OPTIMAL THRESHOLD: 0.70\n",
            "   Maximum F1-Score: 0.4967 (49.67%)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "best_threshold = 0.5\n",
        "best_f1 = 0\n",
        "threshold_results = []\n",
        "\n",
        "print(f\"{'Threshold':<12} {'F1':<10} {'Precision':<12} {'Recall':<10} {'Accuracy':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for threshold in thresholds:\n",
        "    preds = (all_probabilities[:, 1] >= threshold).astype(int)\n",
        "\n",
        "    f1 = f1_score(y_test, preds, pos_label=1)\n",
        "    precision = precision_score(y_test, preds, pos_label=1, zero_division=0)\n",
        "    recall = recall_score(y_test, preds, pos_label=1)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "\n",
        "    threshold_results.append({\n",
        "        'threshold': threshold,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'accuracy': acc\n",
        "    })\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = threshold\n",
        "\n",
        "    marker = \" ‚Üê BEST\" if threshold == best_threshold and f1 == best_f1 else \"\"\n",
        "    print(f\"{threshold:<12.2f} {f1:<10.4f} {precision:<12.4f} {recall:<10.4f} {acc:<10.4f}{marker}\")\n",
        "\n",
        "\n",
        "print(f\"OPTIMAL THRESHOLD: {best_threshold:.2f}\")\n",
        "print(f\"Maximum F1-Score: {best_f1:.4f} ({best_f1*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2128a00d",
      "metadata": {
        "id": "2128a00d"
      },
      "source": [
        "## 15. Final Results with Optimal Threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d949e1",
      "metadata": {
        "id": "01d949e1",
        "outputId": "218d77f5-31df-49b9-c341-f6e811f6d13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FINAL RESULTS (Optimal Threshold: 0.70)\n",
            "================================================================================\n",
            "\n",
            "üìä Performance Metrics:\n",
            "  F1 (Sarcastic):  0.4967 (49.67%) ‚≠ê\n",
            "  F1 (Macro):      0.6995 (69.95%)\n",
            "  Precision:       0.4431\n",
            "  Recall:          0.5650\n",
            "  Accuracy:        0.8364\n",
            "\n",
            "================================================================================\n",
            "Detailed Classification Report:\n",
            "================================================================================\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "Non-Sarcastic (0)     0.9240    0.8817    0.9023      1200\n",
            "    Sarcastic (1)     0.4431    0.5650    0.4967       200\n",
            "\n",
            "         accuracy                         0.8364      1400\n",
            "        macro avg     0.6836    0.7233    0.6995      1400\n",
            "     weighted avg     0.8553    0.8364    0.8444      1400\n",
            "\n",
            "\n",
            "üìä Confusion Matrix:\n",
            "  True Negatives:  1058 (correct non-sarcastic)\n",
            "  False Positives: 142 (non-sarcastic classified as sarcastic)\n",
            "  False Negatives: 87 (sarcastic classified as non-sarcastic)\n",
            "  True Positives:  113 (correct sarcastic) ‚≠ê\n",
            "\n",
            "üí° Minority Class Analysis:\n",
            "  Sarcastic samples in test: 200\n",
            "  Correctly identified: 113 (56.5%)\n",
            "  Precision: 44.3%\n"
          ]
        }
      ],
      "source": [
        "final_predictions = (all_probabilities[:, 1] >= best_threshold).astype(int)\n",
        "\n",
        "final_f1 = f1_score(y_test, final_predictions, pos_label=1)\n",
        "final_precision = precision_score(y_test, final_predictions, pos_label=1)\n",
        "final_recall = recall_score(y_test, final_predictions, pos_label=1)\n",
        "final_accuracy = accuracy_score(y_test, final_predictions)\n",
        "final_f1_macro = f1_score(y_test, final_predictions, average='macro')\n",
        "\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"FINAL RESULTS (Optimal Threshold: {best_threshold:.2f})\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  F1 (Sarcastic):  {final_f1:.4f} ({final_f1*100:.2f}%)\")\n",
        "print(f\"  F1 (Macro):      {final_f1_macro:.4f} ({final_f1_macro*100:.2f}%)\")\n",
        "print(f\"  Precision:       {final_precision:.4f}\")\n",
        "print(f\"  Recall:          {final_recall:.4f}\")\n",
        "print(f\"  Accuracy:        {final_accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "final_cm = confusion_matrix(y_test, final_predictions)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"                  Predicted\")\n",
        "print(f\"                  Non-Sarc  Sarcastic\")\n",
        "print(f\"Actual Non-Sarc     {final_cm[0][0]:4d}      {final_cm[0][1]:4d}\")\n",
        "print(f\"Actual Sarcastic    {final_cm[1][0]:4d}      {final_cm[1][1]:4d}\")\n",
        "\n",
        "print(\"\\n\" + classification_report(\n",
        "    y_test,\n",
        "    final_predictions,\n",
        "    target_names=['Non-Sarcastic (0)', 'Sarcastic (1)'],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "\n",
        "minority_recall = final_cm[1][1] / (final_cm[1][0] + final_cm[1][1])    print(f\"   - Lower the threshold further (try 0.30-0.35)\")\n",
        "\n",
        "minority_precision = final_cm[1][1] / (final_cm[0][1] + final_cm[1][1]) if (final_cm[0][1] + final_cm[1][1]) > 0 else 0    print(f\"   - Train for more epochs (10-12)\")\n",
        "\n",
        "    print(f\"   - Adjust focal_alpha to [0.10, 0.90] for even stronger minority focus\")\n",
        "\n",
        "print(f\"üí° Minority Class (Sarcastic) Analysis:\")    print(f\"   - Increase augmentation factor to 3.0 or 3.5\")\n",
        "\n",
        "print(f\"  Total sarcastic samples in test: {final_cm[1][0] + final_cm[1][1]}\")    print(f\"\\n‚ö†Ô∏è F1 score is {final_f1*100:.1f}%. Tips to improve:\")\n",
        "\n",
        "print(f\"  Correctly identified: {final_cm[1][1]} ({minority_recall*100:.1f}%)\")else:\n",
        "\n",
        "print(f\"  Precision: {minority_precision*100:.1f}%\")    print(f\"\\n‚úÖ SUCCESS: F1 score for minority class is {final_f1*100:.1f}% (target: >50%)\")\n",
        "\n",
        "print(f\"  F1-Score: {final_f1*100:.1f}%\")if final_f1 >= 0.50:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
